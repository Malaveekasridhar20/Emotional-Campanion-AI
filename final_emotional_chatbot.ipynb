{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPqkMvWvfv+5ODLubCynJ07",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Malaveekasridhar20/Emotional-Campanion-AI/blob/main/final_emotional_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xtiu8PWdL3O"
      },
      "outputs": [],
      "source": [
        "# ðŸš€ Install Dependencies\n",
        "!pip install transformers gradio accelerate sentencepiece -q\n",
        "\n",
        "# âœ… Import Libraries\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForCausalLM, pipeline\n",
        "import gradio as gr\n",
        "\n",
        "# === Emotion Detection ===\n",
        "emotion_model_id = \"monologg/bert-base-cased-goemotions-original\"\n",
        "emotion_tokenizer = AutoTokenizer.from_pretrained(emotion_model_id)\n",
        "emotion_model = AutoModelForSequenceClassification.from_pretrained(emotion_model_id)\n",
        "emotion_labels = emotion_model.config.id2label\n",
        "\n",
        "def detect_emotion(text):\n",
        "    inputs = emotion_tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
        "    with torch.no_grad():\n",
        "        logits = emotion_model(**inputs).logits\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "    top_prob, top_idx = torch.max(probs, dim=1)\n",
        "    emotion = emotion_labels[top_idx.item()]\n",
        "    return emotion, top_prob.item()\n",
        "\n",
        "# === Emotionally Intelligent LLM (OpenChat 3.5) ===\n",
        "chat_model_id = \"openchat/openchat-3.5-0106\"\n",
        "chat_tokenizer = AutoTokenizer.from_pretrained(chat_model_id)\n",
        "chat_model = AutoModelForCausalLM.from_pretrained(\n",
        "    chat_model_id,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# === Emotionally Conditioned Reply Function ===\n",
        "def generate_response(user_input, emotion):\n",
        "    system_prompt = f\"\"\"<|system|>\n",
        "You are an emotionally intelligent AI companion.\n",
        "The user is feeling {emotion.upper()}.\n",
        "Provide a warm, helpful, and supportive reply.\n",
        "</s>\n",
        "<|user|>\n",
        "{user_input}\n",
        "<|assistant|>\"\"\"\n",
        "\n",
        "    inputs = chat_tokenizer(system_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        outputs = chat_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=200,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            top_p=0.9\n",
        "        )\n",
        "    response = chat_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response.split(\"<|assistant|>\")[-1].strip()\n",
        "\n",
        "# === Gradio Chat UI ===\n",
        "def emotional_chat(user_message, history=[]):\n",
        "    emotion, prob = detect_emotion(user_message)\n",
        "    reply = generate_response(user_message, emotion)\n",
        "    history.append((f\"[{emotion.upper()}] {user_message}\", reply))\n",
        "    return history, history\n",
        "\n",
        "# === Interface ===\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## ðŸ§  Emotional Companion (OpenChat 3.5)\\nResponds with empathy based on your feelings.\")\n",
        "    chatbot = gr.Chatbot()\n",
        "    msg = gr.Textbox(placeholder=\"Tell me how you're feeling or what happened...\")\n",
        "    state = gr.State([])\n",
        "\n",
        "    def on_submit(message, state):\n",
        "        return emotional_chat(message, state)\n",
        "\n",
        "    msg.submit(on_submit, [msg, state], [chatbot, state])\n",
        "\n",
        "demo.launch()\n"
      ]
    }
  ]
}